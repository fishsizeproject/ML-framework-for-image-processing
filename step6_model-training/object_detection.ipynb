{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Object Detection with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "# !pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
        "# !pip install -q pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check libcudnn8 version\n",
        "!apt-cache policy libcudnn8\n",
        "\n",
        "# Install latest version\n",
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6\n",
        "\n",
        "# Export env variables\n",
        "!export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\n",
        "\n",
        "# Install tensorflow\n",
        "!pip install tflite-model-maker==0.4.0\n",
        "!pip uninstall -y tensorflow && pip install -q tensorflow==2.9.1\n",
        "!pip install pycocotools==2.0.4\n",
        "!pip install opencv-python-headless==4.6.0.66"
      ],
      "metadata": {
        "id": "HMcYtbrC7CPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt -y install libportaudio2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1_Voh48S9Ms",
        "outputId": "0e53394c-3593-413e-ce52-75bbb70d70c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q tflite-model-maker\n",
        "# !pip install -q --use-deprecated=legacy-resolver tflite-model-maker \n"
      ],
      "metadata": {
        "id": "7mq9dXaxS_Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages for object detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Choose an object detection model archiecture"
      ],
      "metadata": {
        "id": "qqqAmZVbDUqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "\n",
        "\n",
        "This tutorial uses the EfficientDet-Lite0 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "\n",
        "**Other models**\n",
        "\n",
        "This library supports EfficientNet-Lite models only. For MobileNetV2 and ResNet50 need to use other libraries. EfficientNet-Lite are a family of image classification models that could achieve state-of-art accuracy and suitable for Edge devices. The default model is EfficientNet-Lite0.\n",
        "\n",
        "We could switch model to MobileNetV2 by just setting parameter model_spec to the MobileNetV2 model specification in create method.\n",
        "\n",
        "List of models names:\n",
        "\n",
        "| Model architecture |\n",
        "|--------------------|\n",
        "| mobilenet_v2 |\n",
        "| efficientdet_lite0 |\n",
        "| resnet_50 |\n",
        "\n",
        "EfficientNet-lite models:\n",
        "\n",
        "https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the dataset"
      ],
      "metadata": {
        "id": "ZdzuEPvADZ-4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "\n",
        "\n",
        "Model Maker will take input data in the CSV format. Use the `object_detector.DataLoader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
        "\n",
        "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
        "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
        "* Test images: These images are used to evaluate the final model performance.\n",
        "\n",
        "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGFijaPGnn-6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q127OWhpnqg7",
        "outputId": "f09c2706-06fe-4010-9669-096141fa1665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://fish_size_project_data/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'fish_size_project_data' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "BUCKET_NAME = 'project_name'\n",
        "\n",
        "GCS_BUCKET = f'gs://{BUCKET_NAME}'\n",
        "!gsutil mb -p $GCP_PROJECT_ID $GCS_BUCKET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8yOKVwnvvj",
        "outputId": "5443a63e-945b-4ea9-e6a0-d119dbc269db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0   107k      0 --:--:-- --:--:-- --:--:--  107k\n",
            "OK\n",
            "53 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 11.6 MB of archives.\n",
            "After this operation, 27.4 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 155645 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.41.4_amd64.deb ...\n",
            "Unpacking gcsfuse (0.41.4) ...\n",
            "Setting up gcsfuse (0.41.4) ...\n"
          ]
        }
      ],
      "source": [
        "# install gcsfuse on colab.\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euq1BjTmnvyF",
        "outputId": "11f92687-34d0-483f-8e44-4cfc3ec1eec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022/06/23 14:22:53.532061 Start gcsfuse/0.41.4 (Go version go1.17.6) for app \"\" using mount point: /content/deeper_data\n",
            "2022/06/23 14:22:53.545162 Opening GCS connection...\n",
            "2022/06/23 14:22:53.793761 Mounting file system \"fish_size_project_data\"...\n",
            "2022/06/23 14:22:53.827133 File system has been successfully mounted.\n"
          ]
        }
      ],
      "source": [
        "!mkdir deeper_data\n",
        "!gcsfuse --implicit-dirs fish_size_project_data deeper_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezRhA5UToAWx",
        "outputId": "13e7ffda-d7c8-418e-d275-95864c0b4ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deeper_data/annotated_images_pilot_study_augmented\n"
          ]
        }
      ],
      "source": [
        "cd deeper_data/annotated_images_pilot_study_augmented/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD5BvzWe6YKa"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('annotations_for_model_6spp_good_qual_augmented.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFP-T4tst2d_",
        "outputId": "195ec5e5-cc4c-4fa4-ec5c-e32e379a8797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 3920 , Test dataset: 512 , Validation dataset: 544\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset:\", train_data.size, \", Test dataset:\", test_data.size, \", Validation dataset:\", validation_data.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiWY8nforTx3",
        "outputId": "b080a60f-f309-4149-b4a4-11ef9ae86a9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "type(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train the TensorFlow model with the training data"
      ],
      "metadata": {
        "id": "v0mA8xsdDizc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "\n",
        "\n",
        "* The EfficientDet-Lite0 model uses `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
        "* Set `batch_size = 8` here so you will see that it takes 21 steps to go through the 175 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "outputs": [],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=32, train_whole_model=False, validation_data=validation_data, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ObFdFQVn-W"
      },
      "source": [
        "**det_loss**:\n",
        "\n",
        "**cls_loss**: a loss that measures the correctness of the classification of each predicted bounding box: each box may contain an object class, or a \"background\". This loss is usually called cross entropy loss.\n",
        "\n",
        "**box_loss**: a loss that measures how \"tight\" the predicted bounding boxes are to the ground truth object (usually a regression loss, L1, smoothL1 etc.).\n",
        "\n",
        "The **learning rate** is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluate the model with the test data"
      ],
      "metadata": {
        "id": "OuCOt9QrDsMP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "\n",
        "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 25 images in the test dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Export as a TensorFlow Lite model"
      ],
      "metadata": {
        "id": "wlJ-MnVvDyQY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='../../', tflite_filename='model_efficientdet_lite0_batch32_epochs20.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Evaluate the TensorFlow Lite model"
      ],
      "metadata": {
        "id": "V6JunA_MD196"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQpahAIBqBPp"
      },
      "source": [
        "\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3Ell_lqH4e"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxaf3x_7OfB"
      },
      "source": [
        "You can download the TensorFlow Lite model file using the left sidebar of Colab. Right-click on the `model.tflite` file and choose `Download` to download it to your local computer.\n",
        "\n",
        "This model can be integrated into an Android or an iOS app using the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview).\n",
        "\n",
        "See the [TFLite Object Detection sample app](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L91) for more details on how the model is used in an working app.\n",
        "\n",
        "*Note: Android Studio Model Binding does not support object detection yet so please use the TensorFlow Lite Task Library.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUAcxnIRKL77"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4u77W5gnzQ"
      },
      "source": [
        "# Read more\n",
        "\n",
        "You can read the [object detection](https://www.tensorflow.org/lite/examples/object_detection/overview) example to learn technical details. For more information, please refer to:\n",
        "\n",
        "*   TensorFlow Lite Model Maker [guide](https://www.tensorflow.org/lite/guide/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "*   Task Library: [ObjectDetector](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) for deployment.\n",
        "*   The end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios), and [Raspberry PI](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "object_detection",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}